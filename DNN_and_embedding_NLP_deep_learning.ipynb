{"cells":[{"cell_type":"markdown","metadata":{"id":"dp5KjlfSST7m"},"source":["# Dense Neural Network and embedding for NLP\n","\n","In this exercise, I explore the use of Dense Neural Network applied to NLP.\n","The goal is to provide an example of NLP classification in the form of sentiment\n","analysis using tensorflow. The general workflow is the same as in the previous exercise but with the addition of embedding. We will look into:\n","\n","- preprocessing\n","- vectorization\n","- embedding\n","- feature extraction and classification using DNN\n","\n","## Preprocessing\n","\n","Text often contains unecessary information or words we may want to modify or remove to help the task and training.  Common preprocessing steps include removing punctuation, lower casing or stop words. Stop words are words that are not useful in the process of classification. The words 'the' or 'is' are very common in English are not usually useful in the process of classification. Care should be taken when using stop words. For instance, if the task is related to sentiment analysis, we should keep 'not' which is also a very common English.\n","The preprocess steps vary and can be  defined before the text vectorization or as part of the vectorization process. When carried out as part of the vectorization, we can pass a function or set specific parameters from the vectorizer (depending on the ML framework used).\n","\n","## Vectorization\n","\n","Before using any ML method, we must first convert the text into numerical values. This is the process of tokenization or vectorization. We can choose different levels for the tokens of the text: characters, words or sentences. In this exercise, we will use word tokenization and will explore sentence tokenization later.\n","\n","The output of this step is typically a tensor/matrix with documents as rows and tokens as columns. There are multiple variations of the matrix (count, frequency, tfidf). In the case of one hot encoding vectorization, the matrix shows the presence of a token for a document. In the case of a frequency count vectorization matrix, it shows the frequency of each words in a document. In most cases, the matrix/tensor is sparse and is processed to reduce its dimension.\n","\n","## Embedding\n","\n","Embedding provides a way to represent text using continuous vector values. It can represent tokens relationship or semantic relations in more efficient way. Vectorization can produce very large sparse and embedding provides more useful ways to represent words in reduced size space. Example of typical embedding include Word2vec and GLOVE. These embeddings were created on very large text dataset and often called pre-trained embedding space. In this exercise, we create our own embedding layer in the deep learning model using the text corpus we have.\n","\n","The text vectorization and embedding steps are sometimes called encoding.\n","\n","## Dense Neural Network: features and classification\n","\n","Embedding layers will feed into intermediary layers that will create features of relevance to the classification. This usually called the feature extractor (backbone) of the DL neural network. The top or head of the network contains the classifier: the neural network corresponding to a binary or multiple category classification.\n","\n","## Dataset\n","\n","In this exercise, we use the Twitter US airline sentiment dataset. It is a public dataset published used in the Kaggle competition published in 2015. It contains about 16k reviews classified into three sentiment categories: negative, neutral and positive.\n","\n","Useful links:\n","\n","- twitter sentiment airlines data:\n","\n","https://www.kaggle.com/datasets/crowdflower/twitter-airline-sentiment\n"]},{"cell_type":"markdown","metadata":{"id":"P0i8hhRib6AU"},"source":["# Set up environment and load libraries\n","\n","- load libraries\n","- install packages and tools\n","- authenticate to google drive and gcp account"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":38068,"status":"ok","timestamp":1709824190182,"user":{"displayName":"Benoit Parmentier","userId":"00838561061844187877"},"user_tz":300},"id":"S1R1WoDsQVnS","outputId":"55643c2f-ea92-4e21-9123-07db0fe76482"},"outputs":[{"output_type":"stream","name":"stdout","text":["Found existing installation: tensorflow 2.13.0\n","Uninstalling tensorflow-2.13.0:\n","  Successfully uninstalled tensorflow-2.13.0\n","Collecting tensorflow==2.13\n","  Using cached tensorflow-2.13.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (524.1 MB)\n","Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13) (1.4.0)\n","Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13) (1.6.3)\n","Requirement already satisfied: flatbuffers>=23.1.21 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13) (23.5.26)\n","Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13) (0.4.0)\n","Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13) (0.2.0)\n","Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13) (1.62.0)\n","Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13) (3.9.0)\n","Requirement already satisfied: keras<2.14,>=2.13.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13) (2.13.1)\n","Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13) (16.0.6)\n","Requirement already satisfied: numpy<=1.24.3,>=1.22 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13) (1.24.3)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13) (3.3.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13) (23.2)\n","Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13) (3.20.3)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13) (67.7.2)\n","Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13) (1.16.0)\n","Requirement already satisfied: tensorboard<2.14,>=2.13 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13) (2.13.0)\n","Requirement already satisfied: tensorflow-estimator<2.14,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13) (2.13.0)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13) (2.4.0)\n","Requirement already satisfied: typing-extensions<4.6.0,>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13) (4.5.0)\n","Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13) (1.14.1)\n","Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13) (0.36.0)\n","Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow==2.13) (0.42.0)\n","Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow==2.13) (2.27.0)\n","Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow==2.13) (1.0.0)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow==2.13) (3.5.2)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow==2.13) (2.31.0)\n","Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow==2.13) (0.7.2)\n","Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow==2.13) (3.0.1)\n","Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow==2.13) (5.3.3)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow==2.13) (0.3.0)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow==2.13) (4.9)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow==2.13) (1.3.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow==2.13) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow==2.13) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow==2.13) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow==2.13) (2024.2.2)\n","Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.14,>=2.13->tensorflow==2.13) (2.1.5)\n","Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow==2.13) (0.5.1)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow==2.13) (3.2.2)\n","Installing collected packages: tensorflow\n","Successfully installed tensorflow-2.13.0\n"]}],"source":["!pip uninstall tensorflow -y\n","!pip install  tensorflow==2.13 #specific version needed for BERT"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":15444,"status":"ok","timestamp":1709824205621,"user":{"displayName":"Benoit Parmentier","userId":"00838561061844187877"},"user_tz":300},"id":"uCU1xstwQakO","outputId":"229f1a10-3f4e-487d-90e9-7e234080ee9b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: np_utils in /usr/local/lib/python3.10/dist-packages (0.6.0)\n","Requirement already satisfied: numpy>=1.0 in /usr/local/lib/python3.10/dist-packages (from np_utils) (1.24.3)\n"]}],"source":["!pip install np_utils"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":12027,"status":"ok","timestamp":1709824217628,"user":{"displayName":"Benoit Parmentier","userId":"00838561061844187877"},"user_tz":300},"id":"4ZJ_11wkD408"},"outputs":[],"source":["###### Library used in this script\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import numpy as np\n","import pandas as pd\n","import os, glob\n","\n","#ML imports\n","import sklearn\n","from sklearn import metrics\n","from sklearn.metrics import roc_auc_score\n","from sklearn.metrics import roc_curve\n","from sklearn.metrics import classification_report\n","from sklearn.metrics import confusion_matrix\n","from sklearn.model_selection import train_test_split\n","from sklearn.decomposition import PCA\n","from sklearn.preprocessing import StandardScaler\n","\n","\n","#Keras import\n","from tensorflow import keras\n","#from keras.models import Model\n","import tensorflow as tf\n","\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras import layers\n","from tensorflow.keras.layers import Input, Conv2D, Concatenate, Activation, MaxPool2D, UpSampling2D, Conv2DTranspose\n","from tensorflow.keras import models\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.utils import plot_model\n","\n","\n","#from keras.utils import np_utils\n","import np_utils\n","sns.set_style('darkgrid')\n","pd.set_option('display.max_colwidth', None)"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":785,"status":"ok","timestamp":1709824218374,"user":{"displayName":"Benoit Parmentier","userId":"00838561061844187877"},"user_tz":300},"id":"loohcADPvP7t","outputId":"86143e37-3998-4bcc-deb5-5e6348accb99"},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":4}],"source":["import seaborn as sns\n","import matplotlib.pyplot as plt\n","import nltk\n","from nltk.tokenize import sent_tokenize, word_tokenize\n","from nltk.corpus import stopwords\n","from nltk.stem import WordNetLemmatizer\n","from collections import Counter\n","\n","import warnings\n","warnings.filterwarnings(\"ignore\")\n","plt.rcParams[\"figure.figsize\"] = (10,6)\n","pd.set_option('display.max_columns', 50)\n","\n","#Please download the below also. This will resolve your issue:\n","\n","nltk.download('punkt')\n","nltk.download('wordnet')\n","nltk.download('stopwords')\n","\n","#nltk.download('omw-1.4'*)"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":173,"status":"ok","timestamp":1709824218541,"user":{"displayName":"Benoit Parmentier","userId":"00838561061844187877"},"user_tz":300},"id":"MSU4mlwHcTLl","outputId":"414cca1a-8138-45fe-fffb-8c599a21dbdc"},"outputs":[{"output_type":"stream","name":"stdout","text":["Authenticated\n"]}],"source":["#GCP account authentification\n","from google.colab import auth\n","auth.authenticate_user()\n","print('Authenticated')"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":61961,"status":"ok","timestamp":1709824280652,"user":{"displayName":"Benoit Parmentier","userId":"00838561061844187877"},"user_tz":300},"id":"meyKA4hlcblK","outputId":"ba6719cf-2aa9-4aee-9f53-157afcaa250c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n"]}],"source":["from google.colab import drive\n","\n","drive.mount('/content/gdrive')"]},{"cell_type":"markdown","metadata":{"id":"Vmy-IfpCD409"},"source":["# Functions\n","In the next part of the script, we declare all the functions used in the sripts.  It is good practice to place functions at the beginning of a script or in an external source file. Here are the 13 functions used:\n","\n","* **create_dir_and_check_existence**:  create and output directory given a path. The output directory will be the working directory throughout the analysis.\n"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":16,"status":"ok","timestamp":1709824280653,"user":{"displayName":"Benoit Parmentier","userId":"00838561061844187877"},"user_tz":300},"id":"OKA3HyN4gFXT"},"outputs":[],"source":["def create_dir_and_check_existence(path):\n","\n","    #Create a new directory\n","    try:\n","        os.makedirs(path)\n","    except:\n","        print (\"directory already exists\")\n","\n","from matplotlib import pyplot"]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":13,"status":"ok","timestamp":1709824280653,"user":{"displayName":"Benoit Parmentier","userId":"00838561061844187877"},"user_tz":300},"id":"IcjPFFLpu6us"},"outputs":[],"source":["def clean_text(data):\n","\n","    import re\n","\n","    #1. Removing URLS\n","    data = re.sub('http\\S+', '', data).strip()\n","    data = re.sub('www\\S+', '', data).strip()\n","\n","    #2. Removing Tags\n","    data = re.sub('#\\S+', '', data).strip()\n","\n","    #3. Removing Mentions\n","    data = re.sub('@\\S+', '', data).strip()\n","\n","    #4. Removing upper brackets to keep negative auxiliary verbs in text\n","    data = data.replace(\"'\", \"\")\n","\n","    #5. Tokenize\n","    text_tokens = word_tokenize(data.lower())\n","\n","    #6. Remove Puncs and number\n","    tokens_without_punc = [w for w in text_tokens if w.isalpha()]\n","\n","    #7. Removing Stopwords\n","    stop_words = stopwords.words('english')\n","    for i in [\"not\", \"no\"]:\n","            stop_words.remove(i)\n","    tokens_without_sw = [t for t in tokens_without_punc if t not in stop_words]\n","\n","    #8. lemma\n","    text_cleaned = [WordNetLemmatizer().lemmatize(t) for t in tokens_without_sw]\n","\n","\n","    #joining\n","    return \" \".join(text_cleaned)"]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":13,"status":"ok","timestamp":1709824280654,"user":{"displayName":"Benoit Parmentier","userId":"00838561061844187877"},"user_tz":300},"id":"4raQR8sNMfPU"},"outputs":[],"source":["#pulled from tensorflow tutorial and more\n","#https://stackoverflow.com/questions/70854665/tensorflow-textvectorization-convert-the-predicted-text-back-to-a-human-readabl\n","\n","def custom_standardization(input_data):\n","  lowercase = tf.strings.lower(input_data)\n","  stripped_html = tf.strings.regex_replace(lowercase, '<\\b>' ,' ')\n","  return tf.strings.regex_replace(stripped_html,'[%s#@]' % re.escape(string.punctuation),'')\n","\n","def normalize(text):\n","  remove_regex = f'[{re.escape(string.punctuation)}]'\n","  space_regex = '...'\n","  result = tf.strings.lower(text)\n","  result = tf.strings.regex_replace(result, remove_regex, '')\n","  result = tf.strings.regex_replace(result, space_regex, ' ')\n","  return result"]},{"cell_type":"code","execution_count":10,"metadata":{"executionInfo":{"elapsed":11,"status":"ok","timestamp":1709824280654,"user":{"displayName":"Benoit Parmentier","userId":"00838561061844187877"},"user_tz":300},"id":"GXlDWRR1PhbG"},"outputs":[],"source":["def generate_accuracy(model,y_test,x_test,class_names=None):\n","\n","  \"\"\"\n","  Function to predict and generate accuracy metrics (precision and recall) for each model given input x and y test.\n","\n","  Extended description of function.\n","\n","  Parameters:\n","  model: keras model\n","  y_test: Input target for test\n","  y_train: target train labels used in classification\n","  class_names: labels for the class predicted\n","\n","  Returns:\n","  report_df: pandas data frame with accuracy metrics\n","\n","  \"\"\"\n","\n","  # we can store the array in a data.frame and get the max\n","  y_score = model.predict(x_test)\n","  y_score.shape\n","  y_pred_test_df = pd.DataFrame(y_score)\n","  y_pred_test_df['y_pred_test'] = y_pred_test_df.idxmax(axis = 1) #one to go across the column for each row\n","  y_pred_test_df\n","\n","\n","  y_pred_test = y_pred_test_df['y_pred_test']\n","  report_dict=classification_report(y_test,\n","                                      y_pred_test,\n","                                      target_names=class_names,\n","                                      output_dict=True)\n","  report_df = pd.DataFrame(report_dict)\n","  #confusion_matrix_val = confusion_matrix(y_test,y_pred_test)  #not in use here\n","  model_name = model._name\n","  #display(report_df)\n","  report_df = (report_df.drop(columns=['macro avg','weighted avg'])\n","                        .drop(labels=['f1-score','support'])\n","                        .assign(model_name= lambda x: model_name)\n","                        .assign(num_param= lambda x: model.count_params())\n","                        .reset_index()\n","                        .rename(columns={'index':'metric'})\n","              )\n","\n","  return report_df"]},{"cell_type":"markdown","metadata":{"id":"dYroYeO_D41D"},"source":["# Parameters and Arguments\n","\n","It is good practice to set all parameters and input arguments at the beginning of the script. This allows for better control and can make modifications of the scripts for other applications easier. Some arguments relate to path directories, input files and general parameters for use in the analyses (e.g. proportion of hold out).\n"]},{"cell_type":"code","execution_count":11,"metadata":{"executionInfo":{"elapsed":10,"status":"ok","timestamp":1709824280654,"user":{"displayName":"Benoit Parmentier","userId":"00838561061844187877"},"user_tz":300},"id":"NcV0IwF3NR1N"},"outputs":[],"source":["############################################################################\n","#####  Parameters and argument set up ###########\n","\n","#ARG 1\n","in_dir = '/content/gdrive/MyDrive/Colab Notebooks/deep-learning-nlp-intro/DNN_and_embedding_nlp/data'\n","out_dir = '/content/gdrive/MyDrive/Colab Notebooks/deep-learning-nlp-intro/DNN_and_embedding_nlp/'\n","\n","#in_filename = 'Tweets.csv'\n","in_filename = 'training.1600000.processed.noemoticon.csv'\n","in_filename_cleaned = 'df_tweets_sentiment140_cleaned.csv'\n","out_suffix = 'DNN_embedding_nlp_2024-03-06'\n","test_proportion = 0.2\n","random_seed= 42\n","create_out_dir = True\n","run_preprocess = False\n","\n","#ARG 7\n","## Input data\n","data_dir = '/content/drive/MyDrive/Colab Notebooks/deep-learning-nlp-intro/DNN_embedding_nlp/data/'\n","#ARG 8\n","run_model = True #if True, model is trained, note this may take several hours.\n","\n","#ARG 9\n","# Use pre-trained model if run_model is False\n","model_path = None\n","#model_path ='/content/drive/MyDrive/Colab Notebooks/deep-learning-nlp-intro/DNN_embedding_nlp/'\n","#ARG 10\n","epoch_val = 100\n","#ARG 11\n","run_preprocess = False\n"]},{"cell_type":"code","execution_count":12,"metadata":{"executionInfo":{"elapsed":2175,"status":"ok","timestamp":1709824282820,"user":{"displayName":"Benoit Parmentier","userId":"00838561061844187877"},"user_tz":300},"id":"cwwPBkLGD41D","colab":{"base_uri":"https://localhost:8080/"},"outputId":"9bd38ae7-a196-4cac-cf25-7438e9bdb594"},"outputs":[{"output_type":"stream","name":"stdout","text":["directory already exists\n"]}],"source":["################# START SCRIPT ###############################\n","\n","######### PART 0: Set up the output dir ################\n","\n","#set up the working directory\n","#Create output directory\n","\n","if create_out_dir==True:\n","    out_dir_new = \"output_data_\"+out_suffix\n","    out_dir = os.path.join(out_dir,\"outputs\",out_dir_new)\n","    create_dir_and_check_existence(out_dir)\n","    os.chdir(out_dir)        #set working directory\n","else:\n","    os.chdir(out_dir) #use working dir defined earlier\n"]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1709824282820,"user":{"displayName":"Benoit Parmentier","userId":"00838561061844187877"},"user_tz":300},"id":"l52WCqGVD41E","outputId":"e2e6a40f-a5db-4232-e66e-3f3a1ceca217"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/gdrive/MyDrive/Colab Notebooks/deep-learning-nlp-intro/DNN_and_embedding_nlp/outputs/output_data_DNN_embedding_nlp_2024-03-06\n"]}],"source":["print(out_dir)"]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":209,"status":"ok","timestamp":1709824283026,"user":{"displayName":"Benoit Parmentier","userId":"00838561061844187877"},"user_tz":300},"id":"ysgioixoy0j0","outputId":"76c1e57b-0225-4f64-b67e-7deba8c72b7f"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/gdrive/MyDrive/Colab Notebooks/deep-learning-nlp-intro/DNN_and_embedding_nlp/outputs/output_data_DNN_embedding_nlp_2024-03-06\n"]}],"source":["!pwd"]},{"cell_type":"markdown","metadata":{"id":"F0pxrb-ZxIHl"},"source":["#0.Workflow/pipeline\n","\n","We describe here the modeling pipelin we set up for this exercise:\n","1. **Load and explore data**\n","- Obtain data from the website or link provided\n","- Quick exploration\n","2. **Intro to Textvectorization and Embedding: tensorflow**\n","- defining layers\n","- hyper parameters\n","3. **Train test split**\n","- create test test split\n","4. **Build and train models**\n","- set up baseline model (random forest)\n","- build DNN model\n","5. **Visualize and Explore fitted embedding**\n","- dimension reduction\n","- visualization\n","6. **Accuracy Assessment**\n","- AUC ROC and precision recall curves\n","- precisions recal metrics\n","- IOU metric\n","7. **Conclusions**\n"]},{"cell_type":"markdown","metadata":{"id":"9YAAEKBBST7s"},"source":["# 1.Load and explore data\n","\n","We used the airlines review sentiment data tweets from Kaggle. There are 14,640 tweets labeled with three sentiment categories:\n","\n","- negative\n","- neutral\n","- positive\n","\n","There were 15 columns including airline name and confidence in the label. Here are the most relevant columns for the modeling:\n","\n","- text: column: contains the tweet text\n","- airline_sentiment: contains the labeled sentiment categories."]},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1709824283026,"user":{"displayName":"Benoit Parmentier","userId":"00838561061844187877"},"user_tz":300},"id":"O69cF891Hu-b","outputId":"ef0b8e2c-69af-42a8-82b3-4596cf16e532"},"outputs":[{"output_type":"stream","name":"stdout","text":["/bin/bash: line 1: nvidia-smi: command not found\n"]}],"source":["# Check for GPU\n","!nvidia-smi -L"]},{"cell_type":"code","execution_count":16,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1709824283026,"user":{"displayName":"Benoit Parmentier","userId":"00838561061844187877"},"user_tz":300},"id":"_m2qO_S8ohHi","outputId":"20697e49-cf32-40c4-baf4-686211542a9d"},"outputs":[{"output_type":"stream","name":"stdout","text":["2.13.0\n"]}],"source":["import tensorflow as tf\n","print(tf.__version__) #should use higher tensorflow!!!"]},{"cell_type":"code","execution_count":17,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":805},"executionInfo":{"elapsed":10786,"status":"ok","timestamp":1709824293809,"user":{"displayName":"Benoit Parmentier","userId":"00838561061844187877"},"user_tz":300},"id":"zy8IdI2Hjcpo","outputId":"70efde9f-02f4-4e0e-cb9d-9a521c4406c9"},"outputs":[{"output_type":"stream","name":"stdout","text":["(1600000, 6)\n","Index(['target', 'ids', 'date', 'flag', 'user', 'text'], dtype='object')\n"]},{"output_type":"execute_result","data":{"text/plain":["         target         ids                          date      flag  \\\n","0             0  1467810369  Mon Apr 06 22:19:45 PDT 2009  NO_QUERY   \n","1             0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY   \n","2             0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY   \n","3             0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n","4             0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n","...         ...         ...                           ...       ...   \n","1599995       4  2193601966  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY   \n","1599996       4  2193601969  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY   \n","1599997       4  2193601991  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY   \n","1599998       4  2193602064  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY   \n","1599999       4  2193602129  Tue Jun 16 08:40:50 PDT 2009  NO_QUERY   \n","\n","                    user  \\\n","0        _TheSpecialOne_   \n","1          scotthamilton   \n","2               mattycus   \n","3                ElleCTF   \n","4                 Karoli   \n","...                  ...   \n","1599995  AmandaMarie1028   \n","1599996      TheWDBoards   \n","1599997           bpbabe   \n","1599998     tinydiamondz   \n","1599999   RyanTrevMorris   \n","\n","                                                                                                                        text  \n","0        @switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D  \n","1            is upset that he can't update his Facebook by texting it... and might cry as a result  School today also. Blah!  \n","2                                  @Kenichan I dived many times for the ball. Managed to save 50%  The rest go out of bounds  \n","3                                                                            my whole body feels itchy and like its on fire   \n","4            @nationwideclass no, it's not behaving at all. i'm mad. why am i here? because I can't see you all over there.   \n","...                                                                                                                      ...  \n","1599995                                                             Just woke up. Having no school is the best feeling ever   \n","1599996                                       TheWDB.com - Very cool to hear old Walt interviews!  â« http://blip.fm/~8bmta  \n","1599997                                                            Are you ready for your MoJo Makeover? Ask me for details   \n","1599998                                                    Happy 38th Birthday to my boo of alll time!!! Tupac Amaru Shakur   \n","1599999                                                       happy #charitytuesday @theNSPCC @SparksCharity @SpeakingUpH4H   \n","\n","[1600000 rows x 6 columns]"],"text/html":["\n","  <div id=\"df-7f5ca2d9-13be-4570-9ffd-d14ae143ec18\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>target</th>\n","      <th>ids</th>\n","      <th>date</th>\n","      <th>flag</th>\n","      <th>user</th>\n","      <th>text</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>1467810369</td>\n","      <td>Mon Apr 06 22:19:45 PDT 2009</td>\n","      <td>NO_QUERY</td>\n","      <td>_TheSpecialOne_</td>\n","      <td>@switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0</td>\n","      <td>1467810672</td>\n","      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n","      <td>NO_QUERY</td>\n","      <td>scotthamilton</td>\n","      <td>is upset that he can't update his Facebook by texting it... and might cry as a result  School today also. Blah!</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0</td>\n","      <td>1467810917</td>\n","      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n","      <td>NO_QUERY</td>\n","      <td>mattycus</td>\n","      <td>@Kenichan I dived many times for the ball. Managed to save 50%  The rest go out of bounds</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0</td>\n","      <td>1467811184</td>\n","      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n","      <td>NO_QUERY</td>\n","      <td>ElleCTF</td>\n","      <td>my whole body feels itchy and like its on fire</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0</td>\n","      <td>1467811193</td>\n","      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n","      <td>NO_QUERY</td>\n","      <td>Karoli</td>\n","      <td>@nationwideclass no, it's not behaving at all. i'm mad. why am i here? because I can't see you all over there.</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>1599995</th>\n","      <td>4</td>\n","      <td>2193601966</td>\n","      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n","      <td>NO_QUERY</td>\n","      <td>AmandaMarie1028</td>\n","      <td>Just woke up. Having no school is the best feeling ever</td>\n","    </tr>\n","    <tr>\n","      <th>1599996</th>\n","      <td>4</td>\n","      <td>2193601969</td>\n","      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n","      <td>NO_QUERY</td>\n","      <td>TheWDBoards</td>\n","      <td>TheWDB.com - Very cool to hear old Walt interviews!  â« http://blip.fm/~8bmta</td>\n","    </tr>\n","    <tr>\n","      <th>1599997</th>\n","      <td>4</td>\n","      <td>2193601991</td>\n","      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n","      <td>NO_QUERY</td>\n","      <td>bpbabe</td>\n","      <td>Are you ready for your MoJo Makeover? Ask me for details</td>\n","    </tr>\n","    <tr>\n","      <th>1599998</th>\n","      <td>4</td>\n","      <td>2193602064</td>\n","      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n","      <td>NO_QUERY</td>\n","      <td>tinydiamondz</td>\n","      <td>Happy 38th Birthday to my boo of alll time!!! Tupac Amaru Shakur</td>\n","    </tr>\n","    <tr>\n","      <th>1599999</th>\n","      <td>4</td>\n","      <td>2193602129</td>\n","      <td>Tue Jun 16 08:40:50 PDT 2009</td>\n","      <td>NO_QUERY</td>\n","      <td>RyanTrevMorris</td>\n","      <td>happy #charitytuesday @theNSPCC @SparksCharity @SpeakingUpH4H</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>1600000 rows × 6 columns</p>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7f5ca2d9-13be-4570-9ffd-d14ae143ec18')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-7f5ca2d9-13be-4570-9ffd-d14ae143ec18 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-7f5ca2d9-13be-4570-9ffd-d14ae143ec18');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-486dd026-2ecb-4d48-903f-c8c0726fe354\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-486dd026-2ecb-4d48-903f-c8c0726fe354')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-486dd026-2ecb-4d48-903f-c8c0726fe354 button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","\n","  <div id=\"id_b92c645a-6eee-4194-8963-b72edf788653\">\n","    <style>\n","      .colab-df-generate {\n","        background-color: #E8F0FE;\n","        border: none;\n","        border-radius: 50%;\n","        cursor: pointer;\n","        display: none;\n","        fill: #1967D2;\n","        height: 32px;\n","        padding: 0 0 0 0;\n","        width: 32px;\n","      }\n","\n","      .colab-df-generate:hover {\n","        background-color: #E2EBFA;\n","        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","        fill: #174EA6;\n","      }\n","\n","      [theme=dark] .colab-df-generate {\n","        background-color: #3B4455;\n","        fill: #D2E3FC;\n","      }\n","\n","      [theme=dark] .colab-df-generate:hover {\n","        background-color: #434B5C;\n","        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","        fill: #FFFFFF;\n","      }\n","    </style>\n","    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df')\"\n","            title=\"Generate code using this dataframe.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n","  </svg>\n","    </button>\n","    <script>\n","      (() => {\n","      const buttonEl =\n","        document.querySelector('#id_b92c645a-6eee-4194-8963-b72edf788653 button.colab-df-generate');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      buttonEl.onclick = () => {\n","        google.colab.notebook.generateWithVariable('df');\n","      }\n","      })();\n","    </script>\n","  </div>\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","variable_name":"df"}},"metadata":{},"execution_count":17}],"source":["#https://www.kaggle.com/datasets/kazanova/sentiment140/code\n","#df = pd.read_csv(os.path.join(in_dir,in_filename),encoding='latin-1')\n","df = pd.read_csv(os.path.join(in_dir,in_filename),header=None,encoding='latin-1')\n","\n","#target: the polarity of the tweet (0 = negative, 2 = neutral, 4 = positive)\n","#ids: The id of the tweet ( 2087)\n","#date: the date of the tweet (Sat May 16 23:58:44 UTC 2009)\n","#flag: The query (lyx). If there is no query, then this value is NO_QUERY.\n","#user: the user that tweeted (robotickilldozr)\n","#text: the text of the tweet (Lyx is cool)\n","\n","column_names = ['target','ids','date','flag','user','text']\n","df.columns = column_names\n","print(df.shape)\n","print(df.columns)\n","df"]},{"cell_type":"markdown","source":[" #(0 = negative, 4 = positive)"],"metadata":{"id":"16vsItONviHn"}},{"cell_type":"code","execution_count":18,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":22,"status":"ok","timestamp":1709824293809,"user":{"displayName":"Benoit Parmentier","userId":"00838561061844187877"},"user_tz":300},"id":"xCBbSHXAh5SY","outputId":"f0e80f08-0720-46d6-de7e-e7c0fb5cf32e"},"outputs":[{"output_type":"stream","name":"stdout","text":["0    800000\n","4    800000\n","Name: target, dtype: int64\n","0    0.5\n","4    0.5\n","Name: target, dtype: float64\n"]}],"source":["print(df['target'].value_counts())\n","print(df['target'].value_counts(normalize=True))\n"]},{"cell_type":"markdown","metadata":{"id":"7O0PucoF-J73"},"source":["The dataset is clearly balanced."]},{"cell_type":"markdown","metadata":{"id":"Y9unQN18pqae"},"source":["#2. **Intro to Textvectorization and Embedding: tensorflow**\n","\n","We will first explore the textvectorization and embedding layers provided in tensorflow keras using a mock up dataset (see corpus below).\n","\n","We'll take a look at:\n","\n","- defining layers: instantiate layers for exploration and fit\n","\n","- hyper parameters: parameters that can be set when using the layers.\n","\n","We'll use a mock-up/fake dataset that consists in 4 sentences with a variety of length with some unusual characters (# and @)."]},{"cell_type":"code","execution_count":19,"metadata":{"executionInfo":{"elapsed":14,"status":"ok","timestamp":1709824293810,"user":{"displayName":"Benoit Parmentier","userId":"00838561061844187877"},"user_tz":300},"id":"6WLuINdXoFh5"},"outputs":[],"source":["corpus = [\n","'Food service was slow and the food was bad and late.',\n","'@airlineZ The flight was very delayed.',\n","'#airlineZ Food was cold and very late.',\n","'I asked for water and coffee but it never came'\n","]"]},{"cell_type":"markdown","metadata":{"id":"PbSBmYr2mkK7"},"source":["## 2.1 Textvectorization"]},{"cell_type":"markdown","metadata":{"id":"BFJe8BmpDYzA"},"source":["Let's use the default parameter values first to explore the vectorization layer. Here is a short description of the main parameters:\n","\n","- max_tokens: maximum number of token to consider for the modeling. It will take most frequent tokens up to the maximum number.\n","- standardize: preprocessing for the input text, this can be a specific option ('lower', 'lower_and_strip_punctuation') or custom function.\n","\n","\n","Intersesting links:\n","- https://jonathan-hui.medium.com/tensorflow-keras-preprocessing-layers-dataset-performance-considera-e9fc11de7bc7: good overview of textvectorization layer\n","\n","- https://discuss.tensorflow.org/t/nlp-textvectorization-tokenizer/12505/3\n","- https://towardsdatascience.com/you-should-try-the-new-tensorflows-textvectorization-layer-a80b3c6b00ee5\n"]},{"cell_type":"code","execution_count":20,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":240,"status":"ok","timestamp":1709824294037,"user":{"displayName":"Benoit Parmentier","userId":"00838561061844187877"},"user_tz":300},"id":"Q9YfB23FzjMm","outputId":"d4d40ee9-e02c-4462-e1ff-ed10455b3c58"},"outputs":[{"output_type":"stream","name":"stdout","text":["<keras.src.layers.preprocessing.text_vectorization.TextVectorization object at 0x7bfe1e521cc0>\n"]}],"source":["#import tensorflow as tf\n","from tensorflow.keras.layers import TextVectorization\n","\n","vectorizer = TextVectorization(max_tokens=None,\n","                              standardize=\"lower_and_strip_punctuation\",\n","                              split=\"whitespace\",\n","                              ngrams=None,\n","                              output_mode=\"int\",\n","                              output_sequence_length=None,\n","                              pad_to_max_tokens=False,\n","                              #vocabulary=corpus,\n","                              idf_weights=None)\n","print(vectorizer)"]},{"cell_type":"markdown","metadata":{"id":"sAaB-a76kSry"},"source":["Here are some additional explanations for the textvectorization layers instantiated with the default parameters:\n","\n","- output_mode: set to 'int', it assigns an integer value for every token\n","- ngrams: set to none by default tokens considered as singleton\n","- outputh_sequence length: set to the longest sequence if None\n","- idf_weights: by default no idf weights are set\n","\n","Once instantiated, we can fit the vectorizer object using 'adapt\" and examine the output using text input:"]},{"cell_type":"code","execution_count":21,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":263,"status":"ok","timestamp":1709824294288,"user":{"displayName":"Benoit Parmentier","userId":"00838561061844187877"},"user_tz":300},"id":"8e-Vjg6YGRzu","outputId":"339c4172-f4d6-459b-b081-dd1520ce75c6"},"outputs":[{"output_type":"stream","name":"stdout","text":["tf.Tensor(\n","[[ 4 11  2 10  3  6  4  2 22  3  7]\n"," [ 8  6 16  2  5 17  0  0  0  0  0]\n"," [ 8  4  2 18  3  5  7  0  0  0  0]\n"," [14 23 15  9  3 19 21 13 12 20  0]], shape=(4, 11), dtype=int64)\n"]}],"source":["vectorizer.adapt(corpus) #fit corpus data\n","print(vectorizer(corpus)) # you can see the padding here.."]},{"cell_type":"markdown","metadata":{"id":"iKkfFCb_lFgV"},"source":["Note the following when examining the outputs once we fit the vectorizer:\n","- int 0: is used for padding i.e. \" \"\n","- int 1: is used for OOV  (out of vocabulary) i.e. 'UNK'\n","- int 2: is the first digit used for the most frequent token (in this case 'was')\n","- int 3: is the next digit used for the next most frequent token (in this case'and')"]},{"cell_type":"markdown","metadata":{"id":"v9EctE3SpnGV"},"source":["Let's tokenize another example sentence to see how the layers works:"]},{"cell_type":"code","execution_count":22,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":89,"status":"ok","timestamp":1709824294288,"user":{"displayName":"Benoit Parmentier","userId":"00838561061844187877"},"user_tz":300},"id":"YK_tzBrPvLym","outputId":"02a7a8d0-ce39-4b0f-f56a-da6f37872057"},"outputs":[{"output_type":"stream","name":"stdout","text":["tf.Tensor([[ 4  2  1  1 22]], shape=(1, 5), dtype=int64)\n"]}],"source":["# Create sample sentence and tokenize it\n","sample_sentence = \"Food was not so bad\"\n","print(vectorizer([sample_sentence]))"]},{"cell_type":"markdown","metadata":{"id":"OjBp51-mp6Jp"},"source":["Note that we have the following tokens matched to the following integers values:\n","\n","- Food: assigned to integer digit '4'\n","- was: assigned to integer digit 2\n","- not: assigned to integer '1' which means OOV (or UNK)\n","- so: assigned to integer '1' which means OOV (or UNK)\n","- bad: assigned to integer digit 22\n","\n","We can see that the tokens 'not' and 'so' were not present in the corpus we used to fit the textvectorizer layer hence they were assigned the digit value '1'."]},{"cell_type":"code","execution_count":23,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":82,"status":"ok","timestamp":1709824294289,"user":{"displayName":"Benoit Parmentier","userId":"00838561061844187877"},"user_tz":300},"id":"FBTv7ixWK-BM","outputId":"380424ef-0450-474f-89b4-456e984f67bf"},"outputs":[{"output_type":"stream","name":"stdout","text":["Count-  25\n"]},{"output_type":"execute_result","data":{"text/plain":["24"]},"metadata":{},"execution_count":23}],"source":["#prints count of unique values\n","vocab_unprocessed = set((' '.join(corpus)).split(\" \"))\n","print(\"Count- \", len(vocab_unprocessed))\n","vectorizer.vocabulary_size() #there are 23 unique words"]},{"cell_type":"code","execution_count":24,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":75,"status":"ok","timestamp":1709824294289,"user":{"displayName":"Benoit Parmentier","userId":"00838561061844187877"},"user_tz":300},"id":"jbWZ1mAOKong","outputId":"0fe05564-71e0-41f0-9c61-da93205f0afa"},"outputs":[{"output_type":"stream","name":"stdout","text":["['', '[UNK]', 'was', 'and', 'food', 'very', 'the', 'late', 'airlinez', 'water']\n","['was', 'and', 'food', 'very', 'the', 'late', 'airlinez', 'water', 'slow', 'service']\n","['i', 'for', 'flight', 'delayed', 'cold', 'coffee', 'came', 'but', 'bad', 'asked']\n","['i', 'for', 'flight', 'delayed', 'cold', 'coffee', 'came', 'but', 'bad', 'asked']\n"]}],"source":["vocab = vectorizer.get_vocabulary()\n","vocab2 = vectorizer.get_vocabulary(include_special_tokens=False)\n","print(vocab[:10])\n","print(vocab2[:10])\n","print(vocab[-10:])\n","print(vocab2[-10:])"]},{"cell_type":"code","execution_count":25,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":68,"status":"ok","timestamp":1709824294290,"user":{"displayName":"Benoit Parmentier","userId":"00838561061844187877"},"user_tz":300},"id":"x9YwdERLikDH","outputId":"f6848640-ab96-445c-ee29-f133081c0211"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(['',\n","  '[UNK]',\n","  'was',\n","  'and',\n","  'food',\n","  'very',\n","  'the',\n","  'late',\n","  'airlinez',\n","  'water',\n","  'slow',\n","  'service',\n","  'never',\n","  'it',\n","  'i',\n","  'for',\n","  'flight',\n","  'delayed',\n","  'cold',\n","  'coffee',\n","  'came',\n","  'but',\n","  'bad',\n","  'asked'],\n"," {'#airlineZ',\n","  '@airlineZ',\n","  'Food',\n","  'I',\n","  'The',\n","  'and',\n","  'asked',\n","  'bad',\n","  'but',\n","  'came',\n","  'coffee',\n","  'cold',\n","  'delayed.',\n","  'flight',\n","  'food',\n","  'for',\n","  'it',\n","  'late.',\n","  'never',\n","  'service',\n","  'slow',\n","  'the',\n","  'very',\n","  'was',\n","  'water'})"]},"metadata":{},"execution_count":25}],"source":["vocab, vocab_unprocessed"]},{"cell_type":"markdown","metadata":{"id":"xSEvXZtAd_7L"},"source":["In the top 10, we have 'was' which appears twice in the corpus. Note that the tokens '' and 'UNK' are also present in the top. This is because the textvectorization layer will put padding '' and the OOV 'UNK' in the list. This implies that these are the most frequent but in fact '' are always put at the top of the list by default. To remove them you can\n","\n","https://github.com/keras-team/keras/blob/v2.13.1/keras/layers/preprocessing/index_lookup.py#L370\n","\n","https://stackoverflow.com/questions/68440502/why-is-unk-word-the-first-in-word2vec-vocabulary\n","\n","\n","TALK ABOUT padding (or mask) and OOV: out of vocabulary"]},{"cell_type":"code","execution_count":26,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":60,"status":"ok","timestamp":1709824294290,"user":{"displayName":"Benoit Parmentier","userId":"00838561061844187877"},"user_tz":300},"id":"hWP9eejcdKZ3","outputId":"7651826a-aa8f-44ad-d8ac-7f290da48a6d"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'#airlineZ',\n"," '@airlineZ',\n"," 'Food',\n"," 'I',\n"," 'The',\n"," 'and',\n"," 'asked',\n"," 'bad',\n"," 'but',\n"," 'came',\n"," 'coffee',\n"," 'cold',\n"," 'delayed.',\n"," 'flight',\n"," 'food',\n"," 'for',\n"," 'it',\n"," 'late.',\n"," 'never',\n"," 'service',\n"," 'slow',\n"," 'the',\n"," 'very',\n"," 'was',\n"," 'water'}"]},"metadata":{},"execution_count":26}],"source":["set((' '.join(corpus)).split(\" \")) #note that @ was removed"]},{"cell_type":"code","execution_count":27,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":51,"status":"ok","timestamp":1709824294290,"user":{"displayName":"Benoit Parmentier","userId":"00838561061844187877"},"user_tz":300},"id":"BWUgJCfMzBJU","outputId":"afd546ac-6577-48b7-a9e8-7e433e43af9e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Number of words in vocab: 24\n","Top 5 most common words: ['', '[UNK]', 'was', 'and', 'food']\n","Bottom 5 least common words: ['coffee', 'came', 'but', 'bad', 'asked']\n"]}],"source":["# Get the unique words in the vocabulary\n","words_in_vocab = vectorizer.get_vocabulary()\n","top_5_words = words_in_vocab[:5] # most common tokens (notice the [UNK] token for \"unknown\" words)\n","bottom_5_words = words_in_vocab[-5:] # least common tokens\n","print(f\"Number of words in vocab: {len(words_in_vocab)}\")\n","print(f\"Top 5 most common words: {top_5_words}\")\n","print(f\"Bottom 5 least common words: {bottom_5_words}\")"]},{"cell_type":"code","execution_count":28,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":43,"status":"ok","timestamp":1709824294290,"user":{"displayName":"Benoit Parmentier","userId":"00838561061844187877"},"user_tz":300},"id":"lEfamb_jIrdP","outputId":"67d628cb-40fa-4c80-c74d-68d094304301"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'name': 'text_vectorization',\n"," 'trainable': True,\n"," 'dtype': 'string',\n"," 'batch_input_shape': (None,),\n"," 'max_tokens': None,\n"," 'standardize': 'lower_and_strip_punctuation',\n"," 'split': 'whitespace',\n"," 'ngrams': None,\n"," 'output_mode': 'int',\n"," 'output_sequence_length': None,\n"," 'pad_to_max_tokens': False,\n"," 'sparse': False,\n"," 'ragged': False,\n"," 'vocabulary': None,\n"," 'idf_weights': None,\n"," 'encoding': 'utf-8',\n"," 'vocabulary_size': 24}"]},"metadata":{},"execution_count":28}],"source":["vectorizer.name\n","vectorizer.get_config()\n","#if output sequence length is None it will be set to the length of the longest sequence in the input text. This may mean a lot of padding."]},{"cell_type":"code","execution_count":29,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":37,"status":"ok","timestamp":1709824294291,"user":{"displayName":"Benoit Parmentier","userId":"00838561061844187877"},"user_tz":300},"id":"1AhIKBFIniNe","outputId":"84528e3a-b3c3-407f-fa4e-6f77e5b2ab49"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["[11, 6, 7, 10]"]},"metadata":{},"execution_count":29}],"source":["[len(sentence.split(\" \")) for sentence in corpus] #this means the output_sequence_length will be set to 11"]},{"cell_type":"code","execution_count":30,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":29,"status":"ok","timestamp":1709824294291,"user":{"displayName":"Benoit Parmentier","userId":"00838561061844187877"},"user_tz":300},"id":"j4nWSpOGzTiO","outputId":"55c6c845-226f-458a-86e3-44ef79f1234e"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tf.Tensor: shape=(1, 7), dtype=int64, numpy=array([[ 8,  4,  2, 18,  3,  5,  7]])>"]},"metadata":{},"execution_count":30}],"source":["# Create sample sentence and tokenize it\n","sample_sentence = corpus[2]\n","vectorizer([sample_sentence])"]},{"cell_type":"markdown","metadata":{"id":"aSuJ1RnQmrrJ"},"source":["## 2.2 Embedding"]},{"cell_type":"markdown","metadata":{"id":"B0stsiaxzjyP"},"source":["There are two required input parameters:\n","- input_dim : size of the vocabulary (number of unique tokens)\n","- output_dim: size of the output embedding space\n","\n","The other arguments have default values. Some important one to know about:\n","- embeddings_initializer: set to uniform by default\n","\n","Useful links to consider:\n","\n","- https://www.tensorflow.org/text/guide/word_embeddings\n","- https://www.tensorflow.org/tutorials/keras/text_classification_with_hub: use pre-trained word embedding\n","- classify text/sentiment with BERT: https://www.tensorflow.org/text/tutorials/classify_text_with_bert\n","- https://towardsdatascience.com/sentiment-analysis-in-10-minutes-with-bert-and-hugging-face-294e8a04b671\n","- pytorch bert fine tuning: https://medium.com/@Mirza_Yusuf/using-a-bert-model-for-sentiment-analysis-6c6fcc106843\n"]},{"cell_type":"code","execution_count":31,"metadata":{"executionInfo":{"elapsed":22,"status":"ok","timestamp":1709824294291,"user":{"displayName":"Benoit Parmentier","userId":"00838561061844187877"},"user_tz":300},"id":"pvSjYTrUzkzO"},"outputs":[],"source":["vocab_size = 24\n","embedding_size = 8\n","embedding = tf.keras.layers.Embedding(\n","               input_dim=vocab_size ,\n","              output_dim=embedding_size,\n","    embeddings_initializer='uniform'\n",")"]},{"cell_type":"code","execution_count":32,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":21,"status":"ok","timestamp":1709824294291,"user":{"displayName":"Benoit Parmentier","userId":"00838561061844187877"},"user_tz":300},"id":"zRmQYKIDVgkD","outputId":"011a61b1-8fd8-45d5-9a75-167a3f63a4ca"},"outputs":[{"output_type":"stream","name":"stdout","text":["(4, 8)\n","tf.Tensor(\n","[[-0.04910794  0.01160494  0.02698851 -0.04816735 -0.03887615  0.04089263\n","  -0.0244019  -0.04835567]\n"," [-0.00060338  0.04162112 -0.04950961 -0.01924855 -0.03785411  0.02670727\n","   0.04302244 -0.02127305]\n"," [ 0.00568552 -0.03523383  0.03330624 -0.02204492 -0.04287448  0.00654\n","   0.03524197  0.00927889]\n"," [ 0.01688966 -0.03843367  0.00253695 -0.0002306   0.04320909 -0.02622211\n","  -0.0007713   0.03621094]], shape=(4, 8), dtype=float32)\n"]}],"source":["print(embedding(tf.constant([10,4,5,9])).shape)\n","print(embedding(tf.constant([10,4,5,9])))\n","#embeding takes input integer values"]},{"cell_type":"markdown","metadata":{"id":"v4G3B2zjV-Le"},"source":["Now let's pass the corpus:"]},{"cell_type":"code","execution_count":33,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":220,"status":"ok","timestamp":1709824294502,"user":{"displayName":"Benoit Parmentier","userId":"00838561061844187877"},"user_tz":300},"id":"S7NXMln1Uz01","outputId":"c6c164fe-4482-4736-e96c-b825d7f4fc2d"},"outputs":[{"output_type":"stream","name":"stdout","text":["(4, 11, 8)\n","(11, 8)\n","tf.Tensor(\n","[[-0.00060338  0.04162112 -0.04950961 -0.01924855 -0.03785411  0.02670727\n","   0.04302244 -0.02127305]\n"," [ 0.0463156  -0.00296923 -0.03524745 -0.02907227 -0.03966471 -0.02930841\n","  -0.02511203  0.00952548]\n"," [-0.03068282 -0.02385294 -0.02588364  0.00982381 -0.01491687 -0.02044965\n","  -0.03391163 -0.01484714]\n"," [-0.04910794  0.01160494  0.02698851 -0.04816735 -0.03887615  0.04089263\n","  -0.0244019  -0.04835567]\n"," [-0.035541   -0.03791713 -0.00772456  0.02585325  0.02381357  0.03533179\n","   0.04261306 -0.04537529]\n"," [-0.00631105 -0.00483658 -0.01395178 -0.00292633 -0.01744783 -0.00520305\n","  -0.02322805  0.00421159]\n"," [-0.00060338  0.04162112 -0.04950961 -0.01924855 -0.03785411  0.02670727\n","   0.04302244 -0.02127305]\n"," [-0.03068282 -0.02385294 -0.02588364  0.00982381 -0.01491687 -0.02044965\n","  -0.03391163 -0.01484714]\n"," [-0.04428315  0.00957913  0.03920485  0.00443207 -0.03336771  0.03481206\n","  -0.00563524  0.04265342]\n"," [-0.035541   -0.03791713 -0.00772456  0.02585325  0.02381357  0.03533179\n","   0.04261306 -0.04537529]\n"," [ 0.04860062  0.03645846 -0.03016238  0.0263822   0.01281459  0.01299554\n","  -0.01007316 -0.01907148]], shape=(11, 8), dtype=float32)\n","Food service was slow and the food was bad and late.\n"]}],"source":["x = embedding(vectorizer(corpus))\n","print(x.shape)\n","print(x[0].shape)\n","print(x[0])\n","print(corpus[0]) #11 tokens (words) with 8 dimensional embedding space"]},{"cell_type":"markdown","metadata":{"id":"xyRn8ZX0CSEH"},"source":["#3. **Train test split and fitting text vectorizer and embedding**\n","\n","Next, we split the airlines tweet data into training and testing. We will also fit the text vectorizer to the training set and create the embedding layer."]},{"cell_type":"markdown","metadata":{"id":"MjnMovRQNAm2"},"source":["## 3.1 Generate train test split"]},{"cell_type":"code","execution_count":34,"metadata":{"executionInfo":{"elapsed":14574,"status":"ok","timestamp":1709824309072,"user":{"displayName":"Benoit Parmentier","userId":"00838561061844187877"},"user_tz":300},"id":"dat9KCrGCHRP"},"outputs":[],"source":["if run_preprocess is True:\n","  cleaned_txt = df[\"text\"].apply(clean_text) #need to fix this step\n","  df[\"text_cleaned\"] = cleaned_txt\n","  del cleaned_txt\n","  df.to_csv('df_tweets_sentiment140_cleaned.csv',index=False)\n","  df[\"text_cleaned\"].head()\n","else:\n","  df = pd.read_csv(os.path.join(in_dir,in_filename_cleaned))\n","  df.drop(columns=['Unnamed: 0'], inplace=True)\n","  df.head()"]},{"cell_type":"code","execution_count":35,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":43,"status":"ok","timestamp":1709824309072,"user":{"displayName":"Benoit Parmentier","userId":"00838561061844187877"},"user_tz":300},"id":"3LmMIJawZ1QH","outputId":"bd75f817-7387-4721-ee2c-9b12efd51fac"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["0    800000\n","1    800000\n","Name: target, dtype: int64"]},"metadata":{},"execution_count":35}],"source":["df['target'].replace(4, 1, inplace=True)\n","df['target'].value_counts()"]},{"cell_type":"code","execution_count":36,"metadata":{"executionInfo":{"elapsed":1350,"status":"ok","timestamp":1709824310387,"user":{"displayName":"Benoit Parmentier","userId":"00838561061844187877"},"user_tz":300},"id":"8sxZwh-LCWZM"},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","df['text_cleaned'] = df['text_cleaned'].astype(str)\n","df['sentiment'] = df['target']\n","X = df['text_cleaned']\n","y = df['target']\n","X_train, X_test, y_train, y_test = train_test_split(X,\n","                                                    y,\n","                                                    test_size=test_proportion,\n","                                                    stratify=y,\n","                                                    random_state=random_seed)"]},{"cell_type":"code","execution_count":37,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":26,"status":"ok","timestamp":1709824310388,"user":{"displayName":"Benoit Parmentier","userId":"00838561061844187877"},"user_tz":300},"id":"tnb55TkiCZrQ","outputId":"2a80dd91-8b14-41ef-85dc-4815c0abb4fa"},"outputs":[{"output_type":"stream","name":"stdout","text":["(1280000,)\n","(320000,)\n","(1280000,)\n","(320000,)\n"]}],"source":["print(X_train.shape)\n","print(X_test.shape)\n","print(y_train.shape)\n","print(y_test.shape)"]},{"cell_type":"code","execution_count":38,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":23,"status":"ok","timestamp":1709824310391,"user":{"displayName":"Benoit Parmentier","userId":"00838561061844187877"},"user_tz":300},"id":"PWq1EX_ZCbKS","outputId":"aa345903-b2d6-4e74-ff54-e3ffdb70d996"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["1036873           lol get idea far advance not even june yet need third knitter summer group\n","287781                                                                   worst headache ever\n","333391                                sad wont see miss already yeah thats perfect come back\n","1484559                                                             doesnt know spell conked\n","562778     quot stand no one know u wont get used wont get used gone quot miss home everyone\n","                                                 ...                                        \n","1592199                                                                     new blog morning\n","880070                                                                       omg amazing job\n","1093760               got home meeting talking endlessly one coolest guy shes ever met smile\n","502113                    bought chocolate bar quot win free bar quot label didnt win either\n","1421597                                                            said hope dm email sunday\n","Name: text_cleaned, Length: 1280000, dtype: object"]},"metadata":{},"execution_count":38}],"source":["X_train"]},{"cell_type":"markdown","metadata":{"id":"Gv7O_ePWEN4l"},"source":["## 3.2 Fit text vectorizer on training samples\n","\n","We create a new textvectorization layer and use the train dataset to fit the layer with default parameters first for exploration."]},{"cell_type":"code","execution_count":39,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":18,"status":"ok","timestamp":1709824310392,"user":{"displayName":"Benoit Parmentier","userId":"00838561061844187877"},"user_tz":300},"id":"30IdUMxAM1HM","outputId":"ea79e49b-9aab-4f16-ad37-d78f65929edf"},"outputs":[{"output_type":"stream","name":"stdout","text":["<keras.src.layers.preprocessing.text_vectorization.TextVectorization object at 0x7bfe1950b7c0>\n"]}],"source":["#import tensorflow as tf\n","from tensorflow.keras.layers import TextVectorization\n","\n","#max_tokens = 15000\n","vectorizer = TextVectorization(max_tokens=None,\n","                              standardize=\"lower_and_strip_punctuation\",\n","                              split=\"whitespace\",\n","                              ngrams=None,\n","                              output_mode=\"int\",\n","                              output_sequence_length=None,\n","                              pad_to_max_tokens=False,\n","                              #vocabulary=corpus,\n","                              idf_weights=None)\n","print(vectorizer)"]},{"cell_type":"code","execution_count":40,"metadata":{"executionInfo":{"elapsed":549070,"status":"error","timestamp":1709824859451,"user":{"displayName":"Benoit Parmentier","userId":"00838561061844187877"},"user_tz":300},"id":"-EXlAE_LMpeJ","colab":{"base_uri":"https://localhost:8080/","height":321},"outputId":"a7be8a97-9085-438b-fca1-3e97837e0075"},"outputs":[{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-40-50c74904b62d>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Fit the text vectorizer to the training text, this can take time: 22 minutes for 1.6 million tweets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madapt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/layers/preprocessing/text_vectorization.py\u001b[0m in \u001b[0;36madapt\u001b[0;34m(self, data, batch_size, steps)\u001b[0m\n\u001b[1;32m    471\u001b[0m               \u001b[0margument\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msupported\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0marray\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    472\u001b[0m         \"\"\"\n\u001b[0;32m--> 473\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madapt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    474\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    475\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mupdate_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/base_preprocessing_layer.py\u001b[0m in \u001b[0;36madapt\u001b[0;34m(self, data, batch_size, steps)\u001b[0m\n\u001b[1;32m    256\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcatch_stop_iteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 258\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_adapt_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    259\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m                         \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    823\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    824\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 825\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    826\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    862\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    863\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 864\u001b[0;31m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_creation_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    865\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_variables\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mALLOW_DYNAMIC_VARIABLE_CREATION\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    866\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    146\u001b[0m       (concrete_function,\n\u001b[1;32m    147\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m--> 148\u001b[0;31m     return concrete_function._call_flat(\n\u001b[0m\u001b[1;32m    149\u001b[0m         filtered_flat_args, captured_inputs=concrete_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs)\u001b[0m\n\u001b[1;32m   1347\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1348\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1349\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_build_call_outputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inference_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1350\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1351\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    194\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mrecord\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_recording\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bound_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m             outputs = self._bound_context.call_function(\n\u001b[0m\u001b[1;32m    197\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m                 \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1455\u001b[0m     \u001b[0mcancellation_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcancellation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1456\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcancellation_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1457\u001b[0;31m       outputs = execute.execute(\n\u001b[0m\u001b[1;32m   1458\u001b[0m           \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1459\u001b[0m           \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     54\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["# Fit the text vectorizer to the training text, this can take time: 22 minutes for 1.6 million tweets\n","vectorizer.adapt(X_train)"]},{"cell_type":"markdown","metadata":{"id":"65AbZa3dvAA-"},"source":["Using the default values we can see that:\n","- the total vocabulary size is 211896 words\n","- the most common words are 'flight', 'not and 'no'.\n","- the least common words seems to contain a lot of typos.\n","-"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":707414,"status":"aborted","timestamp":1709824859451,"user":{"displayName":"Benoit Parmentier","userId":"00838561061844187877"},"user_tz":300},"id":"4GieUs0NPqXw"},"outputs":[],"source":["# Get the unique words in the vocabulary\n","words_in_vocab = vectorizer.get_vocabulary()\n","top_5_words = words_in_vocab[:10] # most common tokens (notice the [UNK] token for \"unknown\" words)\n","bottom_5_words = words_in_vocab[-10:] # least common tokens\n","print(f\"Number of words in vocab: {len(words_in_vocab)}\")\n","print(f\"Top 5 most common words: {top_5_words}\")\n","print(f\"Bottom 5 least common words: {bottom_5_words}\")"]},{"cell_type":"markdown","metadata":{"id":"wQB-dXrut_2s"},"source":["Let's take a look at the size of the sentences (text sequences) in the dataset. We find that on average the text sequences have:\n","- mean of 53 tokens\n","- median of 57 tokens\n","- max of 129\n","- in of 0"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":707412,"status":"aborted","timestamp":1709824859451,"user":{"displayName":"Benoit Parmentier","userId":"00838561061844187877"},"user_tz":300},"id":"sljYWCMgReE2"},"outputs":[],"source":["print(np.mean([len(x_val) for x_val in X_train]))\n","print(np.median([len(x_val) for x_val in X_train]))\n","print(np.max([len(x_val) for x_val in X_train]))\n","print(np.min([len(x_val) for x_val in X_train]))"]},{"cell_type":"markdown","metadata":{"id":"YPIw1l36ufWo"},"source":["Let's create a new layers, this time by setting specific parameters in the layer. We we limit the sequence length to 60 and only keep 5000 words/tokens."]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":707411,"status":"aborted","timestamp":1709824859451,"user":{"displayName":"Benoit Parmentier","userId":"00838561061844187877"},"user_tz":300},"id":"ZhMXaApXROhI"},"outputs":[],"source":["#import tensorflow as tf\n","from tensorflow.keras.layers import TextVectorization\n","\n","max_vocab = 10000\n","sequence_length = 42\n","vectorizer = TextVectorization(max_tokens=max_vocab,\n","                              standardize=\"lower_and_strip_punctuation\",\n","                              split=\"whitespace\",\n","                              ngrams=None,\n","                              output_mode=\"int\",\n","                              output_sequence_length=sequence_length,\n","                              pad_to_max_tokens=False,\n","                              idf_weights=None)\n","print(vectorizer)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oAX00k8sTJTk","executionInfo":{"status":"aborted","timestamp":1709824859452,"user_tz":300,"elapsed":707411,"user":{"displayName":"Benoit Parmentier","userId":"00838561061844187877"}}},"outputs":[],"source":["# Fit the text vectorizer to the training text : 21 minutes\n","vectorizer.adapt(X_train)\n","# Get the unique words in the vocabulary\n","words_in_vocab = vectorizer.get_vocabulary()\n","top_5_words = words_in_vocab[:5] # most common tokens (notice the [UNK] token for \"unknown\" words)\n","bottom_5_words = words_in_vocab[-5:] # least common tokens\n","print(f\"Number of words in vocab: {len(words_in_vocab)}\")\n","print(f\"Top 5 most common words: {top_5_words}\")\n","print(f\"Bottom 5 least common words: {bottom_5_words}\")"]},{"cell_type":"markdown","metadata":{"id":"1MRBvNkaps41"},"source":["#4. **Build and train model**\n","\n","- build DNN model"]},{"cell_type":"markdown","metadata":{"id":"_Qm1z6YrWWWV"},"source":["Let's define model 1 which is a simple fully connected neural network using the dense keras tensorflow layer."]},{"cell_type":"markdown","metadata":{"id":"KsjCBGBnGuql"},"source":["# 4.1 Dense model\n","\n","\n","https://stackoverflow.com/questions/40186722/cast-string-to-float-is-not-supported-in-linear-model\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8eJy9-daWtQK"},"outputs":[],"source":["vocab_size = max_vocab\n","embedding_size = 16\n","num_classes = 2\n","sequence_length = 60 #this is used to set the shape dimension for the vectorizer, see below in the summary"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qH9NA7yOWRLe","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1709765115547,"user_tz":300,"elapsed":19,"user":{"displayName":"Benoit Parmentier","userId":"00838561061844187877"}},"outputId":"76d43f56-8c38-47da-a874-ec41854b1473"},"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"model_dnn\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," input_1 (InputLayer)        [(None, 1)]               0         \n","                                                                 \n"," text_vectorization_2 (Text  (None, 42)                0         \n"," Vectorization)                                                  \n","                                                                 \n"," embedding1 (Embedding)      (None, 42, 16)            160000    \n","                                                                 \n"," dense (Dense)               (None, 42, 8)             136       \n","                                                                 \n"," global_average_pooling1d (  (None, 8)                 0         \n"," GlobalAveragePooling1D)                                         \n","                                                                 \n"," dense_1 (Dense)             (None, 2)                 18        \n","                                                                 \n","=================================================================\n","Total params: 160154 (625.60 KB)\n","Trainable params: 160154 (625.60 KB)\n","Non-trainable params: 0 (0.00 Byte)\n","_________________________________________________________________\n"]}],"source":["inputs = layers.Input(shape=(1,), dtype=\"string\") #1 D array and string as input\n","x = vectorizer(inputs) #this is a layer created earlier\n","x = layers.Embedding(input_dim = vocab_size,\n","                     output_dim = embedding_size,\n","                     name = \"embedding1\")(x)\n","x = layers.Dense(8,activation='relu')(x)\n","x = layers.GlobalAveragePooling1D()(x)\n","outputs = layers.Dense(num_classes, activation=\"softmax\")(x) # create the output layer, want binary outputs so use sigmoid activation\n","model_dnn = tf.keras.Model(inputs, outputs, name= \"dnn_model\") # construct the model\n","\n","model_dnn._name = 'model_dnn' # set keras model name\n","\n","## Compile model\n","model_dnn.compile(optimizer=tf.keras.optimizers.Adam(),\n","              loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n","              metrics=['accuracy'])\n","\n","model_dnn.summary()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ucpd6PLcYJUX","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1709765459619,"user_tz":300,"elapsed":344086,"user":{"displayName":"Benoit Parmentier","userId":"00838561061844187877"}},"outputId":"74a89a00-23dd-43f6-95e6-587374c38cae"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/2\n","8000/8000 [==============================] - 172s 21ms/step - loss: 0.4945 - accuracy: 0.7714 - val_loss: 0.4750 - val_accuracy: 0.7837\n","Epoch 2/2\n","8000/8000 [==============================] - 172s 21ms/step - loss: 0.4739 - accuracy: 0.7865 - val_loss: 0.4740 - val_accuracy: 0.7858\n"]}],"source":["batch_size = 128\n","epochs = 2\n","# Fit the model\n","model_dnn_history = model_dnn.fit(X_train, # input sentences can be a list of strings due to text preprocessing layer built-in model\n","                              y_train,\n","                              batch_size=batch_size,\n","                              epochs=epochs,\n","                              validation_split = 0.2,\n","                              shuffle=True)\n"]},{"cell_type":"markdown","metadata":{"id":"uVuiYRhdG4_F"},"source":["## 4.2 Recurrent LSTM model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AWTnQ28eIdkx"},"outputs":[],"source":["vocab_size = max_vocab\n","embedding_size = 16\n","num_classes = 2"]},{"cell_type":"markdown","source":[" return vector for each word in the Tweet (you can stack RNN cells as long as return_sequences=True)\n","x = layers.LSTM(64)(x)"],"metadata":{"id":"1-3NwDhVsk-b"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"6svuCq0P6Hr5","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1709765459915,"user_tz":300,"elapsed":316,"user":{"displayName":"Benoit Parmentier","userId":"00838561061844187877"}},"outputId":"5ff11193-90c6-4aa6-bebe-410ece0b524a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"model_lstm\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," input_2 (InputLayer)        [(None, 1)]               0         \n","                                                                 \n"," text_vectorization_2 (Text  (None, 42)                0         \n"," Vectorization)                                                  \n","                                                                 \n"," embedding2 (Embedding)      (None, 42, 16)            160000    \n","                                                                 \n"," lstm (LSTM)                 (None, 42, 8)             800       \n","                                                                 \n"," global_average_pooling1d_1  (None, 8)                 0         \n","  (GlobalAveragePooling1D)                                       \n","                                                                 \n"," dense_2 (Dense)             (None, 2)                 18        \n","                                                                 \n","=================================================================\n","Total params: 160818 (628.20 KB)\n","Trainable params: 160818 (628.20 KB)\n","Non-trainable params: 0 (0.00 Byte)\n","_________________________________________________________________\n"]}],"source":["inputs = layers.Input(shape=(1,), dtype=\"string\")\n","x = vectorizer(inputs)\n","x = layers.Embedding(input_dim = vocab_size,\n","                     output_dim = embedding_size,\n","                     name = \"embedding2\")(x)\n","                     # x = layers.LSTM(64, return_sequences=True)(x)\n","\n","\n","x = layers.LSTM(8,return_sequences=True)(x) # return vector for whole sequence\n","x = layers.GlobalAveragePooling1D()(x)\n","outputs = layers.Dense(num_classes, activation=\"softmax\")(x) # create the output layer, want binary outputs so use sigmoid activation\n","model_lstm = tf.keras.Model(inputs, outputs, name= \"lstm_model\") # construct the model\n","\n","model_lstm._name = 'model_lstm' # set keras model name\n","\n","## Compile model\n","model_lstm.compile(optimizer=tf.keras.optimizers.Adam(),\n","              loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n","              metrics=['accuracy'])\n","\n","model_lstm.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BAgFicYDOP0c","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1709765862262,"user_tz":300,"elapsed":402349,"user":{"displayName":"Benoit Parmentier","userId":"00838561061844187877"}},"outputId":"f9b5d78d-3247-488a-fd5b-f10994df393f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/2\n","8000/8000 [==============================] - 202s 25ms/step - loss: 0.4598 - accuracy: 0.7819 - val_loss: 0.4391 - val_accuracy: 0.7950\n","Epoch 2/2\n","8000/8000 [==============================] - 200s 25ms/step - loss: 0.4283 - accuracy: 0.8007 - val_loss: 0.4259 - val_accuracy: 0.8028\n"]}],"source":["# Fit model\n","batch_size =128\n","epochs = 2\n","\n","lstm_history = model_lstm.fit(X_train,\n","                y_train,\n","                batch_size=batch_size,\n","                epochs=epochs,\n","                validation_split = 0.2,\n","                shuffle=True)"]},{"cell_type":"markdown","metadata":{"id":"OkCkT_NuLMFK"},"source":["## 4.3 CNN model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"f5qf_capWVE1","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1709765862262,"user_tz":300,"elapsed":19,"user":{"displayName":"Benoit Parmentier","userId":"00838561061844187877"}},"outputId":"ac16fd98-f07a-4b48-e83a-1151390fb2f1"},"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"model_cnn\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," input_3 (InputLayer)        [(None, 1)]               0         \n","                                                                 \n"," text_vectorization_2 (Text  (None, 42)                0         \n"," Vectorization)                                                  \n","                                                                 \n"," embedding3 (Embedding)      (None, 42, 16)            160000    \n","                                                                 \n"," conv1d (Conv1D)             (None, 38, 16)            1296      \n","                                                                 \n"," global_average_pooling1d_2  (None, 16)                0         \n","  (GlobalAveragePooling1D)                                       \n","                                                                 \n"," dense_3 (Dense)             (None, 2)                 34        \n","                                                                 \n","=================================================================\n","Total params: 161330 (630.20 KB)\n","Trainable params: 161330 (630.20 KB)\n","Non-trainable params: 0 (0.00 Byte)\n","_________________________________________________________________\n"]}],"source":["inputs = layers.Input(shape=(1,), dtype=\"string\")\n","x = vectorizer(inputs)\n","x = layers.Embedding(input_dim = vocab_size,\n","                     output_dim = embedding_size,\n","                     name = \"embedding3\")(x)\n","x = layers.Conv1D(16,kernel_size=5,padding='valid',activation='relu')(x)\n","x = layers.GlobalAveragePooling1D()(x)\n","outputs = layers.Dense(num_classes, activation=\"softmax\")(x) # create the output layer, want binary outputs so use sigmoid activation\n","model_cnn = tf.keras.Model(inputs, outputs, name= \"model_cnn\") # construct the model\n","\n","model_cnn._name = 'model_cnn' # set keras model name\n","\n","## Compile model\n","model_cnn.compile(optimizer=tf.keras.optimizers.Adam(),\n","              loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n","              metrics=['accuracy'])\n","\n","model_cnn.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gsPk6fJ_T8jm","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1709766210830,"user_tz":300,"elapsed":348582,"user":{"displayName":"Benoit Parmentier","userId":"00838561061844187877"}},"outputId":"db6c4c67-741e-4ee1-c924-d85f68db32c5"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/2\n","8000/8000 [==============================] - 175s 22ms/step - loss: 0.4810 - accuracy: 0.7753 - val_loss: 0.4515 - val_accuracy: 0.7928\n","Epoch 2/2\n","8000/8000 [==============================] - 173s 22ms/step - loss: 0.4437 - accuracy: 0.7965 - val_loss: 0.4428 - val_accuracy: 0.7963\n"]}],"source":["# Fit model\n","batch_size =128\n","epochs = 2\n","# Fit model: takes between 50 and 60 minutes for epoch 10, about 10-12 minutes for 2 epochs\n","\n","cnn_history = model_cnn.fit(X_train,\n","                y_train,\n","                batch_size=batch_size,\n","                epochs=epochs,\n","                validation_split = 0.2,\n","                shuffle=True)"]},{"cell_type":"markdown","metadata":{"id":"8mGmP2-oSnjw"},"source":["## 4.4 Training models"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FMJvDYUBSnBb","outputId":"1b4e49e2-0b84-47e4-9736-1efbf252bee3"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/gdrive/MyDrive/Colab Notebooks/deep-learning-nlp-intro/DNN_and_embedding_nlp/outputs/output_data_DNN_embedding_nlp_2024-03-06/model_dnn/checkpoints\n","Epoch 1/50\n","8000/8000 [==============================] - 176s 22ms/step - loss: 0.4723 - accuracy: 0.7872 - val_loss: 0.4740 - val_accuracy: 0.7867\n","Epoch 2/50\n","8000/8000 [==============================] - 173s 22ms/step - loss: 0.4715 - accuracy: 0.7881 - val_loss: 0.4737 - val_accuracy: 0.7868\n","Epoch 3/50\n","8000/8000 [==============================] - 172s 21ms/step - loss: 0.4709 - accuracy: 0.7884 - val_loss: 0.4743 - val_accuracy: 0.7830\n","Epoch 4/50\n","8000/8000 [==============================] - 170s 21ms/step - loss: 0.4704 - accuracy: 0.7886 - val_loss: 0.4732 - val_accuracy: 0.7856\n","Epoch 5/50\n","8000/8000 [==============================] - 172s 22ms/step - loss: 0.4699 - accuracy: 0.7891 - val_loss: 0.4731 - val_accuracy: 0.7869\n","Epoch 6/50\n","8000/8000 [==============================] - 172s 22ms/step - loss: 0.4695 - accuracy: 0.7890 - val_loss: 0.4732 - val_accuracy: 0.7869\n","Epoch 7/50\n","8000/8000 [==============================] - 175s 22ms/step - loss: 0.4692 - accuracy: 0.7893 - val_loss: 0.4731 - val_accuracy: 0.7870\n","Epoch 8/50\n","8000/8000 [==============================] - 174s 22ms/step - loss: 0.4689 - accuracy: 0.7896 - val_loss: 0.4731 - val_accuracy: 0.7879\n","Epoch 9/50\n","8000/8000 [==============================] - 170s 21ms/step - loss: 0.4687 - accuracy: 0.7899 - val_loss: 0.4729 - val_accuracy: 0.7874\n","Epoch 10/50\n","8000/8000 [==============================] - 165s 21ms/step - loss: 0.4685 - accuracy: 0.7897 - val_loss: 0.4729 - val_accuracy: 0.7871\n","Epoch 11/50\n","8000/8000 [==============================] - 165s 21ms/step - loss: 0.4683 - accuracy: 0.7900 - val_loss: 0.4727 - val_accuracy: 0.7870\n","Epoch 12/50\n","8000/8000 [==============================] - 164s 20ms/step - loss: 0.4681 - accuracy: 0.7901 - val_loss: 0.4729 - val_accuracy: 0.7878\n","Epoch 13/50\n","8000/8000 [==============================] - 165s 21ms/step - loss: 0.4680 - accuracy: 0.7901 - val_loss: 0.4728 - val_accuracy: 0.7871\n","Epoch 14/50\n","8000/8000 [==============================] - 166s 21ms/step - loss: 0.4679 - accuracy: 0.7902 - val_loss: 0.4732 - val_accuracy: 0.7859\n","Epoch 15/50\n","8000/8000 [==============================] - 164s 21ms/step - loss: 0.4678 - accuracy: 0.7902 - val_loss: 0.4727 - val_accuracy: 0.7873\n","Epoch 16/50\n","8000/8000 [==============================] - 166s 21ms/step - loss: 0.4677 - accuracy: 0.7903 - val_loss: 0.4727 - val_accuracy: 0.7867\n","Epoch 17/50\n","8000/8000 [==============================] - 165s 21ms/step - loss: 0.4676 - accuracy: 0.7905 - val_loss: 0.4731 - val_accuracy: 0.7858\n","Epoch 18/50\n","8000/8000 [==============================] - 165s 21ms/step - loss: 0.4675 - accuracy: 0.7904 - val_loss: 0.4728 - val_accuracy: 0.7872\n","Saved model to disk  model_dnn\n","/content/gdrive/MyDrive/Colab Notebooks/deep-learning-nlp-intro/DNN_and_embedding_nlp/outputs/output_data_DNN_embedding_nlp_2024-03-06/model_lstm/checkpoints\n","Epoch 1/50\n","8000/8000 [==============================] - 206s 26ms/step - loss: 0.4169 - accuracy: 0.8073 - val_loss: 0.4233 - val_accuracy: 0.8043\n","Epoch 2/50\n","8000/8000 [==============================] - 206s 26ms/step - loss: 0.4100 - accuracy: 0.8109 - val_loss: 0.4227 - val_accuracy: 0.8052\n","Epoch 3/50\n","8000/8000 [==============================] - 199s 25ms/step - loss: 0.4042 - accuracy: 0.8144 - val_loss: 0.4221 - val_accuracy: 0.8051\n","Epoch 4/50\n","8000/8000 [==============================] - 201s 25ms/step - loss: 0.3986 - accuracy: 0.8174 - val_loss: 0.4246 - val_accuracy: 0.8053\n","Epoch 5/50\n","8000/8000 [==============================] - 201s 25ms/step - loss: 0.3931 - accuracy: 0.8205 - val_loss: 0.4235 - val_accuracy: 0.8057\n","Epoch 6/50\n","8000/8000 [==============================] - 197s 25ms/step - loss: 0.3880 - accuracy: 0.8236 - val_loss: 0.4282 - val_accuracy: 0.8053\n","Epoch 7/50\n","8000/8000 [==============================] - 195s 24ms/step - loss: 0.3831 - accuracy: 0.8260 - val_loss: 0.4276 - val_accuracy: 0.8047\n","Epoch 8/50\n","8000/8000 [==============================] - 193s 24ms/step - loss: 0.3787 - accuracy: 0.8287 - val_loss: 0.4316 - val_accuracy: 0.8039\n","Epoch 9/50\n","8000/8000 [==============================] - 182s 23ms/step - loss: 0.3746 - accuracy: 0.8310 - val_loss: 0.4364 - val_accuracy: 0.8037\n","Epoch 10/50\n","8000/8000 [==============================] - 177s 22ms/step - loss: 0.3709 - accuracy: 0.8333 - val_loss: 0.4388 - val_accuracy: 0.8020\n","Epoch 11/50\n","8000/8000 [==============================] - 178s 22ms/step - loss: 0.3676 - accuracy: 0.8350 - val_loss: 0.4383 - val_accuracy: 0.8015\n","Epoch 12/50\n","8000/8000 [==============================] - 179s 22ms/step - loss: 0.3645 - accuracy: 0.8368 - val_loss: 0.4462 - val_accuracy: 0.7990\n","Epoch 13/50\n","8000/8000 [==============================] - 179s 22ms/step - loss: 0.3616 - accuracy: 0.8385 - val_loss: 0.4496 - val_accuracy: 0.7991\n","Epoch 14/50\n","8000/8000 [==============================] - 178s 22ms/step - loss: 0.3590 - accuracy: 0.8400 - val_loss: 0.4554 - val_accuracy: 0.7981\n","Epoch 15/50\n","8000/8000 [==============================] - 178s 22ms/step - loss: 0.3566 - accuracy: 0.8413 - val_loss: 0.4555 - val_accuracy: 0.7980\n","Saved model to disk  model_lstm\n","/content/gdrive/MyDrive/Colab Notebooks/deep-learning-nlp-intro/DNN_and_embedding_nlp/outputs/output_data_DNN_embedding_nlp_2024-03-06/model_cnn/checkpoints\n","Epoch 1/50\n","8000/8000 [==============================] - 161s 20ms/step - loss: 0.4315 - accuracy: 0.8030 - val_loss: 0.4371 - val_accuracy: 0.7997\n","Epoch 2/50\n","8000/8000 [==============================] - 161s 20ms/step - loss: 0.4224 - accuracy: 0.8075 - val_loss: 0.4353 - val_accuracy: 0.8007\n","Epoch 3/50\n","8000/8000 [==============================] - 165s 21ms/step - loss: 0.4151 - accuracy: 0.8115 - val_loss: 0.4355 - val_accuracy: 0.8002\n","Epoch 4/50\n","8000/8000 [==============================] - 164s 21ms/step - loss: 0.4092 - accuracy: 0.8149 - val_loss: 0.4387 - val_accuracy: 0.7992\n","Epoch 5/50\n","8000/8000 [==============================] - 163s 20ms/step - loss: 0.4036 - accuracy: 0.8183 - val_loss: 0.4370 - val_accuracy: 0.8004\n","Epoch 6/50\n","8000/8000 [==============================] - 163s 20ms/step - loss: 0.3983 - accuracy: 0.8211 - val_loss: 0.4401 - val_accuracy: 0.7995\n","Epoch 7/50\n","8000/8000 [==============================] - 163s 20ms/step - loss: 0.3936 - accuracy: 0.8237 - val_loss: 0.4419 - val_accuracy: 0.7984\n","Epoch 8/50\n","8000/8000 [==============================] - 161s 20ms/step - loss: 0.3894 - accuracy: 0.8260 - val_loss: 0.4443 - val_accuracy: 0.7972\n","Epoch 9/50\n","8000/8000 [==============================] - 158s 20ms/step - loss: 0.3850 - accuracy: 0.8282 - val_loss: 0.4471 - val_accuracy: 0.7975\n","Epoch 10/50\n","5296/8000 [==================>...........] - ETA: 47s - loss: 0.3785 - accuracy: 0.8320"]}],"source":["list_models=[model_dnn,model_lstm,model_cnn] #use the model\n","\n","list_hist_df=[]\n","epochs = 50\n","#epochs=2\n","batch_size = 128\n","\n","#if run_model=True then train model and store weights and history\n","if run_model==True:\n","\n","  for model in list_models:\n","\n","    model_name = model._name\n","\n","\n","    callback_early_stopping = keras.callbacks.EarlyStopping(monitor='val_accuracy',\n","                                             patience=10)\n","\n","    SAVE_DIR = os.path.join(out_dir,model_name,'checkpoints')\n","    if not os.path.exists(SAVE_DIR):\n","      os.makedirs(SAVE_DIR)\n","    print(SAVE_DIR)\n","    callback_model_checkpoint = keras.callbacks.ModelCheckpoint(filepath=SAVE_DIR,\n","                                                    monitor='val_accuracy',\n","                                                    save_best_only=True)\n","    history_model = model.fit(X_train,\n","                y_train,\n","                batch_size=batch_size,\n","                epochs=epochs,\n","                validation_split = 0.2,\n","                shuffle=True,\n","                callbacks=[callback_early_stopping,callback_model_checkpoint])\n","\n","    hist_df = pd.DataFrame(history_model.history)\n","\n","    # save to json:\n","    hist_json_file = 'history_'+model_name+'_'+out_suffix+'.json'\n","    with open(hist_json_file, mode='w') as f:\n","        hist_df.to_json(f)\n","    # save to csv:\n","    hist_csv_file = 'history_'+model_name+'_'+out_suffix+'.csv'\n","    with open(hist_csv_file, mode='w') as f:\n","        hist_df.to_csv(f)\n","    model.save(model_name+'_'+out_suffix,save_format='tf')\n","    list_hist_df.append(hist_df)\n","    print(\"Saved model to disk \", model_name)\n","\n","  #if run_model=False then load models weights and loss history from h5 file\n","if run_model==False:\n","  if model_path is None:\n","    model_path = out_dir #classified\n","    print('Set model path to output directory')\n","\n","  model_names=['model_dnn','model_lstm','model_cnn']\n","  i=0\n","  for model_name in model_names:\n","    print(i)\n","    print(model_name)\n","    fileglob =\"*\"+model_name+\"*.h5\"\n","    model_path_filename= os.path.join(model_path,fileglob) #classified\n","    model_path_filename = glob.glob(model_path_filename,recursive=False)[0]\n","    model=list_models[i]\n","    model.summary()\n","    print('This is the model path ',model_path_filename)\n","    model.load_weights(model_path_filename)\n","    fileglob ='*'+model_name+\"*.csv\"\n","    history_model_path = os.path.join(model_path,fileglob) #classified\n","    history_model_path = glob.glob(history_model_path,recursive=False)[0]\n","    hist_df = pd.read_csv(history_model_path)\n","    list_models[i]=model #put the updated model with weights in the list\n","    list_hist_df.append(hist_df)\n","    i=i+1\n","\n","#https://www.tensorflow.org/tutorials/keras/save_and_load"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tH5EAJjJTkaD"},"outputs":[],"source":["print(list_models)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8v2fM6aJgsTM"},"outputs":[],"source":["#history_model_path\n","list_hist_df"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DzIXe4EWncKy"},"outputs":[],"source":["num_models=3\n","fig, ax = plt.subplots(1,num_models,figsize=(16,4))\n","model_names=['model_dnn','model_lstm','model_cnn']\n","\n","for model_val in range(num_models):\n","  hist_df=list_hist_df[model_val]\n","  hist_df['epoch']=np.arange(1,hist_df.shape[0]+1)\n","  ax[model_val].plot(hist_df['epoch'],hist_df['accuracy'],label='train')\n","  ax[model_val].plot(hist_df['epoch'],hist_df['val_accuracy'],label='val')\n","  ax[model_val].set_title(model_names[model_val])\n","  ax[model_val].set_xlabel('epoch')\n","  ax[model_val].set_ylabel('accuracy')\n","  ax[model_val].legend()\n","\n","fig.tight_layout()"]},{"cell_type":"markdown","metadata":{"id":"dn47iQGYWCXJ"},"source":["#6. Accuracy Assessement and evaluation of models"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eoDpc1GkU0aL"},"outputs":[],"source":["#model_use.predict(X_test)\n","y_score = model_cnn.predict(X_test)\n","print(type(y_score))\n","y_score.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0rOZVgARUpMK"},"outputs":[],"source":["y_pred_test_df = pd.DataFrame(y_score)\n","y_pred_test_df['y_pred_test'] = y_pred_test_df.idxmax(axis = 1) #one to go across the column for each row\n","\n","y_pred_test_df"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Bj7o0jlMUukH"},"outputs":[],"source":["#from sklearn.metrics import classification_report\n","#from sklearn.metrics import confusion_matrix\n","y_pred_test = y_pred_test_df['y_pred_test']\n","report_dict=classification_report(y_test,\n","                                      y_pred_test,\n","                                      output_dict=True)\n","report_df = pd.DataFrame(report_dict).transpose()\n","confusion_matrix_val = confusion_matrix(y_test,y_pred_test)\n","\n","fig, ax = plt.subplots(figsize=(16,16))\n","ax.matshow(confusion_matrix_val,cmap=plt.cm.Blues,alpha=0.3)\n","for i in range(confusion_matrix_val.shape[0]):\n","  for j in range(confusion_matrix_val.shape[1]):\n","    ax.text(x=j,y=i,\n","               s=confusion_matrix_val[i,j],\n","               va='center',\n","               ha='center')\n","    plt.xlabel('Predicted Label')\n","    plt.ylabel('True label')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"f9a1X65FU0bY"},"outputs":[],"source":["report_df"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"moVbsD_RU6DB"},"outputs":[],"source":["list_report_df = []\n","#class_names=['negative','neutral','positive']\n","class_names=['negative','positive']\n","\n","for model in list_models:\n","  report_df = generate_accuracy(model,y_test,X_test,class_names=class_names )\n","  list_report_df.append(report_df)\n","#list_report_df\n","\n","from functools import reduce\n","report_df = reduce(lambda df1,df2: df1.merge(df2,\"outer\"),list_report_df)\n","report_df"]},{"cell_type":"markdown","metadata":{"id":"3MwFGFyQHA36"},"source":["#7.Conclusions\n","\n","https://datascience.stackexchange.com/questions/13490/how-to-set-class-weights-for-imbalanced-classes-in-keras\n","\n","https://www.kaggle.com/code/arunrk7/nlp-beginner-text-classification-using-lstm\n","\n","\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"H4LvTYX3cVbw"},"source":["#8. References\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-KuNbMaQD41Y"},"outputs":[],"source":["############################# END OF SCRIPT ###################################"]}],"metadata":{"accelerator":"TPU","colab":{"machine_shape":"hm","provenance":[{"file_id":"1Wr2AhlOSVo4odVOB2azhUERVa15pkrTR","timestamp":1685131001017},{"file_id":"1M_9QISTnTR3j8IVpnSm36b3o0O6DJB4g","timestamp":1617744706386},{"file_id":"1iNq4Od3piGMamfcLC2kgclSOHI302YPb","timestamp":1615586855694},{"file_id":"1gSQFWYqH_NbSo9duH41TzDmmGOyeayMj","timestamp":1615341489443},{"file_id":"1Qozn-kJ6FCZzxtH95eWC6ZrQXYfz9MDx","timestamp":1615129952394},{"file_id":"1BbwQKpjKYJ-cX41V_doI4jFArR4x8Gxq","timestamp":1614393885639},{"file_id":"1pjeHtof1UjaWVAyj92a6rENesSTKky-E","timestamp":1613960426366},{"file_id":"1DUJSKBq4_2kYA0l-SIxwtv2kLNI5PG9v","timestamp":1613450424070},{"file_id":"1QKzpwQvQLO_JRwISWNNI1V7YFmr3Sdvv","timestamp":1612638702093},{"file_id":"1MnAFYREcpcop1URkfNf_5Irndob0wHN2","timestamp":1612224403543}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"}},"nbformat":4,"nbformat_minor":0}